{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "470a44b2",
        "outputId": "466f0ded-3d4d-42d8-8865-ca049561d5c4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#load the california housing data\n",
        "df = pd.read_csv(\"housing.csv\") \n",
        "\n",
        "df.describe()\n",
        "df.info()\n",
        "\n",
        "\n",
        "#total_bedrooms      20433 non-null  float64 - this feaure has missing values\n",
        "#verified using df.isnull().sum()\n",
        "\n",
        "#Fix it by, every missing total_bedrooms value was filled using the median value of that column’s values. median is used to rebust the skewness ( this column's data is skewed data) and outliers. \n",
        "df['total_bedrooms'] = df['total_bedrooms'].fillna(df['total_bedrooms'].median())\n",
        "df.isnull().sum()\n",
        "\n",
        "#To find which feature has largest variance\n",
        "df = df.select_dtypes(include='number')\n",
        "all_vars = df.var()\n",
        "all_vars.idxmax(), all_vars.max() #ANSWER: median_house_value has largest variance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#TASK2: \n",
        "# Univariate Analysis (Histograms)\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#--------------------------------------------------------------------------------------------------------------#\n",
        "\n",
        "df.hist(bins=20, figsize=(15,10)) #Tried with different bin sizes-> 30, 40, 80..\n",
        "plt.suptitle(\"Histograms of Numeric Features (bins = 30)\", fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "#OBSERVATIONS FROM THE OUTPUT HISTOGRAM PLOTS:\n",
        "'''\n",
        "->Most of the features are skewed (long right tails/ more concentration at the lower values) -> plots of total_rooms, total_bedrooms, population, households are right skewed.\n",
        "->This shows that features are non-normal distributed.\n",
        "->The plots of longitude and latitude explains that most houses are located in two different regions.\n",
        "\n",
        "METHODS TO ELIMINATE THE SKEWNESS.\n",
        "->Skewness can be reduced by log transformation (log(x+1)), this method is best for right skewed data, it compresses larger values(positive values).\n",
        "->squre-root(sqrt(x)): This method is also useful for positively skewed data, it stables the variance and makes the data similar to normally distributed which makes the analysis easier.\n",
        "->box-cox, yeo-johnson(handles both positive and negative values) transforms can be used to remove skewness\n",
        "->Trimming: we can remove the extreme outliers which are contributing significantly towards the skewness.\n",
        "'''\n",
        "#--------------------------------------------------------------------------------------------------------------#\n",
        "#BOX plot\n",
        "#create AveRooms field -> total rooms/ household\n",
        "df['AveRooms'] = df['total_rooms'] / df['households']\n",
        "\n",
        "plt.figure(figsize=(12,10))\n",
        "sns.boxplot(data=df[['median_income', 'AveRooms', 'population']])\n",
        "plt.title(\"Box Plot for Outliers Detection\")\n",
        "plt.ylabel(\"Value\")\n",
        "plt.show()\n",
        "#OBSERVATIONS FROM THE BOX PLOT:\n",
        "''' \n",
        "->Population has very high spread and is strongly right skewed, with many extreme high outliers\n",
        "->Median_income and AveRooms show much smaller variability compared with population.\n",
        "->Median_income and AveRooms both have only a few outliers, so their distributions are relatively compact and stable.\n",
        "->Because population has many extreme high values, the model may overfocus on those rare cases and give less accurate predictions for the majority of normal observation.\n",
        "'''\n",
        "#---------------------------------------------------------------------------------------------------------------#\n",
        "\n",
        "#correlation Heatmap\n",
        "plt.figure(figsize=(10,8))\n",
        "#correlation matrix\n",
        "corr_mat = df.corr()\n",
        "\n",
        "#heatmap of correlation matrix\n",
        "sns.heatmap(corr_mat, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "plt.show()\n",
        "#---------------------------------------------------------------------------------------------------------------#\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Latitude/Longitude Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#SCATTER PLOT: Longitude vs Latitude\n",
        "plt.figure(figsize=(12,10))\n",
        "scatter = plt.scatter(df['longitude'], df['latitude'], c=df['median_house_value'], s=df['population']/100, cmap='viridis', alpha=0.5)\n",
        "plt.colorbar(scatter, label='Median House Value')\n",
        "plt.xlabel(\"Longitude\")\n",
        "plt.ylabel(\"Latitude\")\n",
        "plt.title(\"California Housing Prices\")\n",
        "plt.show()\n",
        "#obervation: Housing prices in California are higher near the coast and lower in inland areas, showing the strong impact of geographical location.\n",
        "#Areas with higher population density (larger dots) are mostly found near the coast."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#TASK: Explain why scaling is required before PCA\n",
        "'''\n",
        "->PCA works by finding directions where the data varies the most. If one feature has very large values (like population) and another has small values (like median income), PCA will focus more on the large one.\n",
        "->Without scaling, features with larger values dominate, even if they are not more important. This gives biased and incorrect principal components.\n",
        "->Scaling puts all features on the same scale, so each feature gets a fair chance to contribute to PCA results.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Apply PCA on features\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "#Step 1: scale the data (IMPORTANT STEP)\n",
        "scaler = StandardScaler()\n",
        "\n",
        "#we dropped med house value becaouse it is the output varibale, where the models learns from the data and produces the output, we med value isn't removed then model already knows the output and give false high accuracy outcome.\n",
        "X = df.drop('median_house_value', axis=1)\n",
        "medhouseval = df['median_house_value']\n",
        "\n",
        "X_is_scaled = scaler.fit_transform(X)\n",
        "\n",
        "#Step 2: now apply pca\n",
        "pca = PCA()\n",
        "pca_applied = pca.fit_transform(X_is_scaled)\n",
        "\n",
        "#plot explained variance ratio\n",
        "var_ratio = pca.explained_variance_ratio_ #indiviadual var per principle component\n",
        "cum_var = np.cumsum(var_ratio)            #cumulative var\n",
        "\n",
        "plt.figure(figsize=(12,5))\n",
        "\n",
        "#subplot1 individual variance \n",
        "plt.subplot(1,2,1)\n",
        "plt.bar(range(1,len(var_ratio)+1), var_ratio,alpha = 0.6, color='skyblue', edgecolor='navy')\n",
        "plt.xlabel('Principal Components')\n",
        "plt.ylabel('Individual Explained variance ratio')\n",
        "plt.title('Individual Variance per PC')\n",
        "plt.xticks(range(1,len(var_ratio)+1))\n",
        "\n",
        "#subplot2 cumulative variance\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(1, len(cum_var)+1), cum_var, 'ro-', linewidth=2, markersize=7)\n",
        "plt.axhline(y=0.8, color='b', linestyle='--', label='80% threshold')\n",
        "plt.axhline(y=0.95, color='orange', linestyle='--', label='95% threshold')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.title('Cumulative Variance')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\"\"\"\n",
        "DIFFERENCE BETWEEN INDIVIDUAL vs CUMULATIVE:\n",
        "\n",
        "INDIVIDUAL:\n",
        "-> Shows % of TOTAL variance captured by EACH principal component\n",
        "-> PC1 captures 37%, PC2 18%, PC3 12%,...\n",
        "-> First few PCs are most important individually.\n",
        "-> We can identify which single components matter most\n",
        "\n",
        "CUMULATIVE:\n",
        "-> Running total: PC1 + PC2 + ... + PCn\n",
        "-> Shows % of TOTAL variance captured by first n components\n",
        "-> Helps decide \"how many PCs needed for 80/90% variance\"\n",
        "-> Use case: Dimensionality reduction decisions\n",
        "\n",
        "example:\n",
        "-> individual: [0.37, 0.18, 0.12, 0.08, ...] \n",
        "-> Cumulative: [0.37, 0.55, 0.67, 0.75, ...] -> Here 2 PCs are needed for 55% variance\n",
        "\"\"\"\n",
        "#----------------------------------------------------------------------------------------------#\n",
        "\n",
        "#choose top 2 components\n",
        "pca2 = PCA(n_components=2)\n",
        "data_pca2 = pca2.fit_transform(X_is_scaled)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(data_pca2[:,0], data_pca2[:,1], c=df['median_house_value'], cmap='plasma', alpha=0.6)\n",
        "plt.colorbar(label='Median House Value')\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "plt.title(\"PCA Projection\")\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#MULTIPLE LINEAR REGRESSION\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LinearRegression\n",
        "#---------------------------------------------------------------------------------#\n",
        "#1. Why we Excluded latitude and longitude Features?\n",
        "#Justification for why we are excluding lat/long using Correlation matrix\n",
        "corr_matrix = df.corr()\n",
        "print(\"Correlation with median_house_value:\")\n",
        "relevant_corrs = corr_matrix['median_house_value'].sort_values(ascending=False)\n",
        "print(relevant_corrs)\n",
        "\n",
        "#correlation heatmap\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, fmt='.2f')\n",
        "plt.title('Correlation Matrix - Lat/Long have LOW correlation with house value')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\"\"\"\n",
        "-> Med Incom: +0.68 (STRONG positive -> Imp predictor)\n",
        "-> Latitude: -0.14 (WEAK negative)\n",
        "-> Longitude: -0.05 (VERY WEAK negative) \n",
        "-> Lat,Long encode geography, not causal price factors\n",
        "-> Hence we exclude them for interpretable economic model\n",
        "\"\"\"\n",
        "#----------------------------------------------------------------------------------#\n",
        "\n",
        "features = df.drop(['median_house_value','latitude', 'longitude'], axis=1)\n",
        "target_variable = df['median_house_value']\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, target_variable, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"\\nTraining set and Test set: {X_train.shape}, {X_test.shape}\")\n",
        "#create a pipeline\n",
        "linreg_pipeline = Pipeline([('scaler', StandardScaler()), ('linreg_model', LinearRegression())])\n",
        "linreg_pipeline.fit(X_train, y_train)\n",
        "\n",
        "#print coefficients-intercept matrix\n",
        "coefficients = linreg_pipeline.named_steps['linreg_model'].coef_\n",
        "intercept = linreg_pipeline.named_steps['linreg_model'].intercept_\n",
        "\n",
        "coef_df = pd.DataFrame({'Feature': features.columns, 'Coefficient':coefficients})\n",
        "intercept_df = pd.DataFrame({'Feature':['Intercept'], 'Coefficient':[intercept]})\n",
        "coef_intercept_matrix = pd.concat([coef_df, intercept_df], ignore_index=True)\n",
        "print(coef_intercept_matrix)\n",
        "#---------------------------------------------------------------------------------#\n",
        "\n",
        "#evaluation of model on metrics of mse, mae loss, R^2 score , adjusted R^2 score\n",
        "y_pred = linreg_pipeline.predict(X_test)\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "n = X_test.shape[0] #number of data points\n",
        "k = X_test.shape[1] #number of features(Independent variables)\n",
        "adj_r2 = 1 - ((1-r2)*(n-1)/(n-k-1))\n",
        "\n",
        "print(f\"MSE: {mse}\")\n",
        "print(f\"MAE: {mae}\")\n",
        "print(f\"R²: {r2}\")\n",
        "print(f\"Adjusted R²: {adj_r2}\")\n",
        "\n",
        "\"\"\" \n",
        "IS HIGH R2 SCORE ALWAYS GOOD?\n",
        "ANSWER: NO, Sometimes a high R square value can be misleadiing and it may also indicates overfitting. The model may work well on training data but it will fail when we give new data. The important point to to generalise the model to perform well on unseen data rather than the data it is trained on\n",
        "\n",
        "IS LOW TRAINING LOSS ALWAYS PREFERRED?\n",
        "ANSWER: NO, low training loss means the odel may be learning noise.\n",
        "low training loss and high validation loss is a sign of overfitting.\n",
        "\"\"\"\n",
        "\n",
        "#plots: Predicted vs actual \n",
        "plt.figure(figsize=(6,6))\n",
        "plt.scatter(y_test, y_pred, alpha=0.6)\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
        "plt.xlabel(\"Actual Values\")\n",
        "plt.ylabel(\"Predicted Values\")\n",
        "plt.title(\"Predicted vs Actual\")\n",
        "plt.show()\n",
        "\n",
        "#plot: residuals vs Predicted values\n",
        "residuals = y_test - y_pred\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.scatter(y_pred, residuals, alpha=0.6)\n",
        "plt.axhline(0, color='red', linestyle='--')\n",
        "plt.xlabel(\"Predicted Values\")\n",
        "plt.ylabel(\"Residuals\")\n",
        "plt.title(\"Residuals vs Predicted\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#BONUS PART\n",
        "#TRAIN RIDGE(L2 Regularization) AND LASSO(L1 Regularization) REGRESSION \n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "\n",
        "x = df.drop('median_house_value', axis = 1)\n",
        "y = df['median_house_value']"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
